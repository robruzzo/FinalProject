{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Robert E Ruzzo III\n",
    "DoubleLayer.ipynb\n",
    "\n",
    "The purpose of this notebook is for prototyping various model structures for processing of the \n",
    "Histopathological Cancer Detection Dataset obtained from Kaggle.com\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dropout, Input\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "import os\n",
    "from keras import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration\n",
    "Used to hold variable values making them easier to change if needed.\n",
    "\n",
    "    Args: \n",
    "        None\n",
    "\n",
    "Variables:\n",
    "    batch_size (int): The batch processing size\n",
    "    epochs (int):  The number epoch iterations to run on the model\n",
    "    data (string): The location of the training set labels csv\n",
    "    data_dir (string): The directory which containes the subdirectories of the photos to be analzed. \n",
    "        For this notebook to work correctly the pictures have to divided into a sub directories based on their class.\n",
    "    image_width (int): Expected width of the pictures (enforced by the training and validation generators)\n",
    "    image_height (int): Expected height of the pictures (enforced byt he training and validation generators)\n",
    "    center_crop_width (int): The desired width of the image after it has been cropped, used in the cropping generator\n",
    "    center_crop_height (int): The desired height of the image after it has been cropped, used in the cropping generator\n",
    "    model_name (string): The name of the model for both TensorBoard callbacks and saving of weights\n",
    "\n",
    "\"\"\"\n",
    "class Configuration:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 40\n",
    "        self.data = pd.read_csv('/floyd/input/cancer_histo/train_labels.csv')\n",
    "        self.data_dir = '/floyd/input/cancer_histo/train'\n",
    "        #Set up cropping variables\n",
    "        self.image_width = 96\n",
    "        self.image_height = 96\n",
    "        self.center_crop_width = 96\n",
    "        self.center_crop_height = 96 \n",
    "        self.model_name = \"DoubleLayers_Crop96_40\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the configuration class\n",
    "config = Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see that the image/label data has loaded\n",
    "config.data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the file that causes an exception\n",
    "config.data=config.data[config.data.id != 'b44ceb87f4fb92169ec928c652d6e1209b48135c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function builds the model using keras and tensorflow attributes\n",
    "\n",
    "    Args:\n",
    "       None\n",
    "\n",
    "    Returns:\n",
    "        TensorFlow model object with the desired attributes\n",
    "\n",
    "    \"\"\"\n",
    "def build_model():\n",
    "    inputs = Input(shape=(config.center_crop_width,config.center_crop_height,3), name=\"input\")\n",
    "    \n",
    "    #Convolution 1\n",
    "    conv1 = Conv2D(256, kernel_size=(3,3), activation=\"relu\", name=\"conv_1\")(inputs)\n",
    "    conv2 = Conv2D(256, kernel_size=(3,3), activation=\"relu\", name=\"conv_2\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), name=\"pool_1\")(conv2)\n",
    "\n",
    "    #Convolution 2\n",
    "    conv4 = Conv2D(128, kernel_size=(3,3), activation=\"relu\", name=\"conv_4\")(pool1)\n",
    "    conv5 = Conv2D(128, kernel_size=(3,3), activation=\"relu\", name=\"conv_5\")(conv4)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2), name=\"pool_2\")(conv5)\n",
    "    \n",
    "    #Convolution 3\n",
    "    conv6 = Conv2D(64, kernel_size=(3,3), activation=\"relu\", name=\"conv_6\")(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2), name=\"pool_3\")(conv6)\n",
    "    \n",
    "       \n",
    "    #Fully Connected Layer\n",
    "    flatten = Flatten()(pool3)\n",
    "    fc1 = Dense(1024, activation=\"relu\", name=\"fc_1\")(flatten)\n",
    "    \n",
    "    #output\n",
    "    output=Dense(2, activation=\"softmax\", name =\"softmax\")(fc1)\n",
    "    #Using the ADAM optimizer with a learning rate of 0.001\n",
    "    adm = optimizers.adam(lr=0.001)\n",
    "    # finalize and compile\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=adm, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks are created allowing us to monitor via TensorBoard\n",
    "def create_callbacks(name):\n",
    "\n",
    "    tensorboard_callback = TensorBoard(log_dir=os.path.join(os.getcwd(), \"tensorboard_log\", name), write_graph=True, write_grads=False)\n",
    "    \n",
    "    return [tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cropping function \n",
    "\n",
    "    Args:\n",
    "        img (image): An image to be cropped\n",
    "        crop_size_x (int): As defined in the config class, width of final image\n",
    "        crop_size_y (int): As defined in the config class, height of final image\n",
    "\n",
    "    Returns:\n",
    "        An image, cropped to the specified values\n",
    "    \"\"\"\n",
    "#Define a center croping function to use along with the cropping generator\n",
    "def center_crop(img, crop_size_x, crop_size_y):\n",
    "    height, width =img.shape[0],img.shape[1]\n",
    "    crop_x_start = (width-crop_size_x)//2\n",
    "    crop_x_end =(crop_x_start + crop_size_x)\n",
    "    crop_y_start = (height-crop_size_y)//2\n",
    "    crop_y_end =(crop_y_start + crop_size_y)\n",
    "    return img[(crop_y_start):(crop_y_end), (crop_x_start):(crop_x_end)]\n",
    "\n",
    "\"\"\"Cropping Generator\n",
    "\n",
    "    Args:\n",
    "        batches (iterable generator object) : A batch which gets iterated through\n",
    "        crop_size_x (int): As defined in the config class, width of final image\n",
    "        crop_size_y (int): As defined in the config class, height of final image\n",
    "\n",
    "    Returns:\n",
    "        batch_cropped (iterable batch of images): The images that have been cropped\n",
    "        batch_y (int) : number of items in the batch\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "#Cropping generator, takes iterable generator output as an input and crops images (in memory) to the desired size\n",
    "def crop_generator(batches, crop_size_x, crop_size_y):\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_cropped = np.zeros((batch_x.shape[0], crop_size_x, crop_size_y, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_cropped[i] = center_crop(batch_x[i], crop_size_x, crop_size_y)\n",
    "        yield (batch_cropped, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"setup_data - Function creates the generators which add data variance and cropping capabilities\n",
    "    Note: This function has to return the number of items in the iterables to ensure functionality with the \n",
    "    fit function.\n",
    "\n",
    "    Args:\n",
    "        train_data_dir (string) : directory that the training and validation data are located\n",
    "        batch_size (int) : size of the batches (count)\n",
    "\n",
    "    Returns:\n",
    "        traing_cropped (iterable image generator) : cropped and augmented training images\n",
    "        validation_cropped (iterable image generator) : cropped and augmented validation images\n",
    "        train_generator.n (int) : The number of items in the training generator iterable\n",
    "        validation_generator.n (int): The number of items in the validation generator iterable\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "#Setup data, and create split for training, testing 80/20\n",
    "def setup_data(train_data_dir, batch_size):\n",
    "    #Shear and zoom of 20% variation are added to the images for for varying data\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2) # set validation split\n",
    "    \n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(96,96), #target size is added to ensure that the largest a file can be is 96 x 96 pix\n",
    "        class_mode='categorical', #class mode will make a separate class determination for each subdirectory\n",
    "        subset='training')\n",
    "    \n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(96,96), #target size is added to ensure that the largest a file can be is 96 x 96 pix\n",
    "        class_mode='categorical', #class mode will make a separate class determination for each subdirectory\n",
    "        subset='validation')\n",
    "    \n",
    "    training_cropped = crop_generator(train_generator,config.center_crop_width, config.center_crop_height)\n",
    "    validation_cropped = crop_generator(validation_generator,config.center_crop_width, config.center_crop_height)\n",
    "    \n",
    "    return training_cropped, validation_cropped, train_generator.n, validation_generator.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"fit_model : This function uses the iterable generators to build and train the model, as well as return the \n",
    "    output of the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (Keras/TensorFlow model object) : The model created with build_model function\n",
    "        train_generator (iterable image generator object) : The iterable training generator from the setup_data function\n",
    "        val_generator (iterable image generator object) : The iterable validation generator from the setup_data function\n",
    "        batch_size (int) : The batch size, or number of objects processed with each batch iteration.\n",
    "        epochs (int) : The number of total iterations through the data\n",
    "        name (string) : The name of the model for TensorBoard Callbacks\n",
    "        \n",
    "    Returns:\n",
    "        model (model object) : A tensorflow / Keras model definition with weights and structure data included\n",
    "\n",
    "    \"\"\"\n",
    "def fit_model(model, train_generator, val_generator, batch_size, epochs, name):\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=training_n // batch_size + 1,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_n // batch_size + 1,\n",
    "        callbacks=create_callbacks(name=name),\n",
    "        verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"eval_model : This function uses the output of fit_model to evaluate the model after training is complete,\n",
    "    and shows validation accuracy and validation loss as parameters.\n",
    "\n",
    "    Args:\n",
    "        model (Keras/TensorFlow model object) : The trained model output from fit_model\n",
    "        val_generator (iterable image generator object) : The iterable validation generator from the setup_data function\n",
    "        batch_size (int) : The batch size, or number of objects processed with each batch iteration.\n",
    "        \n",
    "    Returns:\n",
    "        None, output is printed\n",
    "\n",
    "    \"\"\"\n",
    "def eval_model(model, val_generator, batch_size):\n",
    "    scores = model.evaluate_generator(val_generator, steps=val_n // batch_size+1)\n",
    "    print(\"Loss: \" + str(scores[0]) + \" Accuracy: \" + str(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup data generators\n",
    "train_generator, val_generator, training_n, val_n= setup_data(config.data_dir, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a model object using the model defined in build_model, the print the output\n",
    "model = build_model()\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model, note the device_name and with direct the usage of a GPU for processing\n",
    "device_name=\"/gpu:0\"\n",
    "with tf.device(device_name):\n",
    "    model = fit_model(model, train_generator, val_generator,\n",
    "                      batch_size=config.batch_size,\n",
    "                      epochs=config.epochs,\n",
    "                      name=config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "device_name=\"/gpu:0\"\n",
    "with tf.device(device_name):\n",
    "    eval_model(model, val_generator, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "model.save(config.model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
