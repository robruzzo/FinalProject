{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Robert E Ruzzo III\n",
    "Resnet50.ipynb\n",
    "\n",
    "The purpose of this notebook is for implementing the ResNet50 model on the \n",
    "Histopathological Cancer Detection Dataset obtained from Kaggle.com\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dropout, Input,GlobalAveragePooling2D\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "import os\n",
    "from keras import metrics\n",
    "from PIL import Image \n",
    "from keras import applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration\n",
    "Used to hold variable values making them easier to change if needed.\n",
    "\n",
    "    Args: \n",
    "        None\n",
    "\n",
    "Variables:\n",
    "    batch_size (int): The batch processing size\n",
    "    epochs (int):  The number epoch iterations to run on the model\n",
    "    data (string): The location of the training set labels csv\n",
    "    data_dir (string): The directory which containes the subdirectories of the photos to be analzed. \n",
    "        For this notebook to work correctly the pictures have to divided into a sub directories based on their class.\n",
    "    image_width (int): Expected width of the pictures (enforced by the training and validation generators)\n",
    "    image_height (int): Expected height of the pictures (enforced byt he training and validation generators)\n",
    "    center_crop_width (int): The desired width of the image after it has been cropped, used in the cropping generator\n",
    "    center_crop_height (int): The desired height of the image after it has been cropped, used in the cropping generator\n",
    "    name (string): The name of the model for both TensorBoard callbacks and saving of weights\n",
    "\n",
    "\"\"\"\n",
    "class Configuration:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 10\n",
    "        self.data = pd.read_csv('D:\\\\Datasets\\\\histopathologic-cancer-detection\\\\train_labels.csv')\n",
    "        self.data_dir = 'D:\\\\Datasets\\\\histopathologic-cancer-detection\\\\train\\\\'\n",
    "        self.image_width = 96\n",
    "        self.image_height = 96\n",
    "        self.name='ResNet50_10_BCross'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of the configuration class\n",
    "config = Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the id of an image that throws exceptions on opening\n",
    "config.data=config.data[config.data.id != 'b44ceb87f4fb92169ec928c652d6e1209b48135c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the ResNet50 Model Structure\n",
    "num_classes=2\n",
    "adm = optimizers.adam(lr=0.0001)\n",
    "model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (config.image_width,config.image_height,3))\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(num_classes, activation= 'softmax')(x)\n",
    "model_50 = Model(inputs = model.input, outputs = predictions)\n",
    "model_50.compile(optimizer=adm, loss='binary_crossentropy', metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates callbacks to monitor on Tensorboard\n",
    "def create_callbacks(name):\n",
    "    tensorboard_callback = TensorBoard(log_dir=os.path.join(os.getcwd(), \"tensorboard_log\", name), write_graph=True, write_grads=False)\n",
    "    return [tensorboard_callback] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"setup_data - Function creates the generators which add data variance and cropping capabilities\n",
    "    Note: This function has to return the number of items in the iterables to ensure functionality with the \n",
    "    fit function.\n",
    "\n",
    "    Args:\n",
    "        train_data_dir (string) : directory that the training and validation data are located\n",
    "        batch_size (int) : size of the batches (count)\n",
    "\n",
    "    Returns:\n",
    "        traing_cropped (iterable image generator) : cropped and augmented training images\n",
    "        validation_cropped (iterable image generator) : cropped and augmented validation images\n",
    "        train_generator.n (int) : The number of items in the training generator iterable\n",
    "        validation_generator.n (int): The number of items in the validation generator iterable\n",
    "\n",
    "    \"\"\"\n",
    "#Setup data, and create split for training, testing 80/20\n",
    "def setup_data(train_data_dir, batch_size):\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.1) # set validation split\n",
    "    \n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(96,96),\n",
    "        class_mode='categorical',\n",
    "        subset='training')\n",
    "    \n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(96,96),\n",
    "        class_mode='categorical',\n",
    "        subset='validation')\n",
    "    \n",
    "    return train_generator, validation_generator, train_generator.n, validation_generator.n\n",
    "    #return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"fit_model : This function uses the iterable generators to build and train the model, as well as return the \n",
    "    output of the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (Keras/TensorFlow model object) : The model created with build_model function\n",
    "        train_generator (iterable image generator object) : The iterable training generator from the setup_data function\n",
    "        val_generator (iterable image generator object) : The iterable validation generator from the setup_data function\n",
    "        batch_size (int) : The batch size, or number of objects processed with each batch iteration.\n",
    "        epochs (int) : The number of total iterations through the data\n",
    "        name (string) : The name of the model for TensorBoard Callbacks\n",
    "        \n",
    "    Returns:\n",
    "        model (model object) : A tensorflow / Keras model definition with weights and structure data included\n",
    "\n",
    "    \"\"\"\n",
    "def fit_model(model, train_generator, val_generator, batch_size, epochs, name):\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=training_n // batch_size +1,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_n // batch_size +1,\n",
    "        callbacks=create_callbacks(name=name),\n",
    "        verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"eval_model : This function uses the output of fit_model to evaluate the model after training is complete,\n",
    "    and shows validation accuracy and validation loss as parameters.\n",
    "\n",
    "    Args:\n",
    "        model (Keras/TensorFlow model object) : The trained model output from fit_model\n",
    "        val_generator (iterable image generator object) : The iterable validation generator from the setup_data function\n",
    "        batch_size (int) : The batch size, or number of objects processed with each batch iteration.\n",
    "        \n",
    "    Returns:\n",
    "        None, output is printed\n",
    "\n",
    "    \"\"\"\n",
    "def eval_model(model, val_generator, batch_size):\n",
    "    scores = model.evaluate_generator(val_generator, steps=val_n // batch_size+1)\n",
    "    print(\"Loss: \" + str(scores[0]) + \" Accuracy: \" + str(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 198024 images belonging to 2 classes.\n",
      "Found 22001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of the necessary generators\n",
    "train_generator, val_generator, training_n, val_n= setup_data(config.data_dir, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print the model summary (uncomment to view)\n",
    "#print (model_50.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1548/1548 [==============================] - 1422s 919ms/step - loss: 0.4425 - categorical_accuracy: 0.7998 - val_loss: 0.4064 - val_categorical_accuracy: 0.8216\n",
      "Epoch 2/10\n",
      "1548/1548 [==============================] - 1463s 945ms/step - loss: 0.3835 - categorical_accuracy: 0.8297 - val_loss: 0.4222 - val_categorical_accuracy: 0.8328\n",
      "Epoch 3/10\n",
      "1548/1548 [==============================] - 1459s 942ms/step - loss: 0.3504 - categorical_accuracy: 0.8465 - val_loss: 0.3450 - val_categorical_accuracy: 0.8546\n",
      "Epoch 4/10\n",
      "1548/1548 [==============================] - 1410s 911ms/step - loss: 0.3173 - categorical_accuracy: 0.8645 - val_loss: 0.4926 - val_categorical_accuracy: 0.7690\n",
      "Epoch 5/10\n",
      "1548/1548 [==============================] - 1410s 911ms/step - loss: 0.2880 - categorical_accuracy: 0.8785 - val_loss: 0.5743 - val_categorical_accuracy: 0.7958\n",
      "Epoch 6/10\n",
      "1548/1548 [==============================] - 1411s 911ms/step - loss: 0.2608 - categorical_accuracy: 0.8918 - val_loss: 0.3240 - val_categorical_accuracy: 0.8887\n",
      "Epoch 7/10\n",
      "1548/1548 [==============================] - 1430s 924ms/step - loss: 0.2390 - categorical_accuracy: 0.9028 - val_loss: 0.3177 - val_categorical_accuracy: 0.8592\n",
      "Epoch 8/10\n",
      "1548/1548 [==============================] - 1420s 917ms/step - loss: 0.2224 - categorical_accuracy: 0.9099 - val_loss: 0.2262 - val_categorical_accuracy: 0.9095\n",
      "Epoch 9/10\n",
      "1548/1548 [==============================] - 1412s 912ms/step - loss: 0.2103 - categorical_accuracy: 0.9169 - val_loss: 0.2755 - val_categorical_accuracy: 0.8822\n",
      "Epoch 10/10\n",
      "1548/1548 [==============================] - 1413s 913ms/step - loss: 0.1939 - categorical_accuracy: 0.9233 - val_loss: 0.2250 - val_categorical_accuracy: 0.9105\n"
     ]
    }
   ],
   "source": [
    "#Run the model, using a gpu\n",
    "device_name=\"/gpu:0\"\n",
    "with tf.device(device_name):\n",
    "    model_out = fit_model(model_50, train_generator, val_generator,\n",
    "                      config.batch_size,\n",
    "                      config.epochs,\n",
    "                      config.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model.\n",
    "device_name=\"/gpu:0\"\n",
    "with tf.device(device_name):\n",
    "    eval_model(model, val_generator, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model weights\n",
    "model.save(config.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
